{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjQjH27HZJ1/Xn++YBIC1p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohithanagineni/Semi-supervised-and-supervised-learning-methods-for-detecting-fake-online-reviews/blob/main/final_code_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF6BdURBAVkP",
        "outputId": "f945fb9c-ab5f-4adc-9b8d-e7b72ec44bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Data preprocessing completed.\n",
            "Number of training samples: 1482\n",
            "Number of validation samples: 317\n",
            "Number of testing samples: 319\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/dataset/UCMerced_LandUse/Images'  # Directory containing the 21 folders\n",
        "img_width, img_height = 150, 150  # Desired image dimensions\n",
        "\n",
        "# List all the class labels\n",
        "class_labels = os.listdir(data_dir)\n",
        "\n",
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each folder\n",
        "for class_label in class_labels:\n",
        "    folder_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_files = os.listdir(folder_path)\n",
        "\n",
        "        # Load and resize the images, and store the labels\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.resize(img, (img_width, img_height))\n",
        "            images.append(img)\n",
        "            labels.append(class_label)\n",
        "\n",
        "# Convert the images and labels to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Perform label encoding on the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Perform one-hot encoding on the labels\n",
        "num_classes = len(class_labels)\n",
        "labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Shuffle the data\n",
        "indices = np.arange(len(images))\n",
        "np.random.shuffle(indices)\n",
        "images = images[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "# Perform data normalization\n",
        "images = images.astype('float32') / 255.0\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_split = 0.7  # Percentage of data for training\n",
        "val_split = 0.15  # Percentage of data for validation\n",
        "test_split = 0.15  # Percentage of data for testing\n",
        "\n",
        "num_samples = len(images)\n",
        "num_train_samples = int(train_split * num_samples)\n",
        "num_val_samples = int(val_split * num_samples)\n",
        "\n",
        "x_train = images[:num_train_samples]\n",
        "y_train = labels[:num_train_samples]\n",
        "\n",
        "x_val = images[num_train_samples:num_train_samples + num_val_samples]\n",
        "y_val = labels[num_train_samples:num_train_samples + num_val_samples]\n",
        "\n",
        "x_test = images[num_train_samples + num_val_samples:]\n",
        "y_test = labels[num_train_samples + num_val_samples:]\n",
        "\n",
        "print(\"Data preprocessing completed.\")\n",
        "print(\"Number of training samples:\", len(x_train))\n",
        "print(\"Number of validation samples:\", len(x_val))\n",
        "print(\"Number of testing samples:\", len(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/dataset/UCMerced_LandUse/Images'  # Directory containing the 21 folders\n",
        "img_width, img_height = 150, 150  # Desired image dimensions\n",
        "num_classes = 21  # Number of land use classes\n",
        "\n",
        "# List all the class labels\n",
        "class_labels = os.listdir(data_dir)\n",
        "\n",
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each folder\n",
        "for class_label in class_labels:\n",
        "    folder_path = os.path.join(data_dir, class_label)\n",
        "\n",
        "    if os.path.isdir(folder_path):\n",
        "        image_files = os.listdir(folder_path)\n",
        "\n",
        "        # Load and resize the images, and store the labels\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(folder_path, image_file)\n",
        "            img = cv2.imread(image_path)\n",
        "            img = cv2.resize(img, (img_width, img_height))\n",
        "            images.append(img)\n",
        "            labels.append(class_label)\n",
        "\n",
        "# Convert the images and labels to NumPy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Perform label encoding on the class labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Perform one-hot encoding on the labels\n",
        "labels = to_categorical(labels, num_classes=num_classes)\n",
        "\n",
        "# Perform data normalization\n",
        "images = images.astype('float32') / 255.0\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_split = 0.7  # Percentage of data for training\n",
        "val_split = 0.15  # Percentage of data for validation\n",
        "test_split = 0.15  # Percentage of data for testing\n",
        "\n",
        "num_samples = len(images)\n",
        "num_train_samples = int(train_split * num_samples)\n",
        "num_val_samples = int(val_split * num_samples)\n",
        "\n",
        "x_train = images[:num_train_samples]\n",
        "y_train = labels[:num_train_samples]\n",
        "\n",
        "x_val = images[num_train_samples:num_train_samples + num_val_samples]\n",
        "y_val = labels[num_train_samples:num_train_samples + num_val_samples]\n",
        "\n",
        "x_test = images[num_train_samples + num_val_samples:]\n",
        "y_test = labels[num_train_samples + num_val_samples:]\n",
        "\n",
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Load the pre-trained VGG16 model\n",
        "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Freeze the layers of the VGG16 model\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model by combining the VGG16 model and the CNN model\n",
        "combined_model = Sequential()\n",
        "combined_model.add(vgg_model)\n",
        "combined_model.add(Flatten())\n",
        "combined_model.add(Dense(128, activation='relu'))\n",
        "combined_model.add(Dropout(0.5))\n",
        "combined_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the combined model\n",
        "combined_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the combined model\n",
        "combined_model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = combined_model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGBLtnraCLOE",
        "outputId": "f67f44f2-b960-4b9f-e694-32a2b2e27238"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "47/47 [==============================] - 563s 12s/step - loss: 2.5160 - accuracy: 0.2497 - val_loss: 4.3346 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/10\n",
            "47/47 [==============================] - 556s 12s/step - loss: 1.4975 - accuracy: 0.5816 - val_loss: 4.5328 - val_accuracy: 0.0063\n",
            "Epoch 3/10\n",
            "47/47 [==============================] - 501s 11s/step - loss: 1.0864 - accuracy: 0.7072 - val_loss: 4.9982 - val_accuracy: 0.0063\n",
            "Epoch 4/10\n",
            "47/47 [==============================] - 561s 12s/step - loss: 0.8533 - accuracy: 0.7706 - val_loss: 5.2637 - val_accuracy: 0.0063\n",
            "Epoch 5/10\n",
            "47/47 [==============================] - 557s 12s/step - loss: 0.7373 - accuracy: 0.8104 - val_loss: 5.4756 - val_accuracy: 0.0189\n",
            "Epoch 6/10\n",
            "47/47 [==============================] - 559s 12s/step - loss: 0.6179 - accuracy: 0.8266 - val_loss: 5.6821 - val_accuracy: 0.0126\n",
            "Epoch 7/10\n",
            "47/47 [==============================] - 558s 12s/step - loss: 0.5470 - accuracy: 0.8489 - val_loss: 5.9179 - val_accuracy: 0.0158\n",
            "Epoch 8/10\n",
            "47/47 [==============================] - 558s 12s/step - loss: 0.4806 - accuracy: 0.8752 - val_loss: 5.8452 - val_accuracy: 0.0379\n",
            "Epoch 9/10\n",
            "47/47 [==============================] - 501s 11s/step - loss: 0.4223 - accuracy: 0.8954 - val_loss: 6.1662 - val_accuracy: 0.0252\n",
            "Epoch 10/10\n",
            "47/47 [==============================] - 549s 12s/step - loss: 0.3851 - accuracy: 0.8995 - val_loss: 6.3686 - val_accuracy: 0.0347\n",
            "10/10 [==============================] - 85s 8s/step - loss: 7.2590 - accuracy: 0.0000e+00\n",
            "Test Loss: 7.258962631225586\n",
            "Test Accuracy: 0.0\n"
          ]
        }
      ]
    }
  ]
}